## FinChain: The Chat-Driven Expense Tracker (Full Stack Â· Chatbot Â· NLP Analytics Â· Pinecone Semantic Search Â· FAST API)


**Imagine:** What if you could track and analyze your expenses just by _chatting in your own language_? No clunky dropdowns, no boring forms, no explicit category selection. Just say what you spent money onâ€”**in any language, any style**â€”{Type â€œSpent 670 on burgers todayâ€, â€œà¦¬à¦¾à¦°à§à¦—à¦¾à¦° à¦–à¦¾à¦‡à¦¸à¦¿ à§§à§¦à§¦à§¦ à¦Ÿà¦¾à¦•à¦¾ à¦¦à¦¿à§Ÿà¦¾â€, or â€œUber ride last night: 450â€â€”FinChain extracts item, amount, and category, all powered by an LLM-based NLP backend] and FinChainâ€™s AI does the rest: parses, extracts, categorizes, and gives you beautiful analytics. No more "finance apps" that you quit after two weeks. This is expense tracking, re-invented for real people.

---

-![Dashboard](https://github.com/user-attachments/assets/b647fbb8-bc3b-4800-bf7e-5f3921ccadad)

<p align="center">
![Spending Categories](https://github.com/user-attachments/assets/d7e07ea9-0803-4953-b370-765bba5f5443)
![Transaction Reports](https://github.com/user-attachments/assets/3b134dda-ab24-4aa4-bac6-88913e092723)
</p>


## âœ¨ Features

- **Chat Your Expenses, Any Language:**  
  Type _â€œSpent 670 on burgers todayâ€_, _â€œà¦¬à¦¾à¦°à§à¦—à¦¾à¦° à¦–à¦¾à¦‡à¦¸à¦¿ à§§à§¦à§¦à§¦ à¦Ÿà¦¾à¦•à¦¾ à¦¦à¦¿à§Ÿà¦¾â€_, or _â€œUber ride last night: 450â€_â€”FinChain extracts item, amount, and category, all powered by an LLM-based NLP backend.
- **No Manual Categorization:**  
  You never have to pick categories, tags, or fill in forms. FinChain understands your text, auto-categorizes, and keeps learning.
- **Full-Stack Power:**
  - **Backend:** FastAPI (Python), async REST, MongoDB (expense storage), Pinecone (vector/semantic search for future AI queries).
  - **Frontend:** Next.js (React), Tailwind, Recharts, Lucide iconsâ€”pixel-perfect, modern dashboard UI.
  - **AI/NLP:** Hugging Face Transformers, LangChain, ONNX+TensorRT (GPU optimized inference coming soon!)
- **Beautiful Analytics:**
  - Auto-generated pie/bar charts, totals, monthly breakdowns.
  - â€œLatest transactionsâ€ view with full NLP parsing results.
  - Filter, search, and semantic queries planned.
 
## ğŸ› ï¸ Tech Stack

### ğŸ”§ Backend
- **FastAPI** (Python)
- **LangChain** (LLM)
- **MongoDB**
- **Pinecone** (Vector DB for semantic search)
- **Docker** (for deployment)

### ğŸ–¥ï¸ Frontend
- **Next.js** (React)
- **Tailwind CSS**
- **Recharts** (for charts & graphs)

### ğŸ“¡ APIs
- **RESTful endpoints**:
  - `/parse_expense`
  - `/expenses`
  - `/analytics/category`
  - `/analytics/monthly`
  - `/analytics/total`
  - `/query_expenses`

### ğŸ¤– NLP / LLMs
- Current: **Groq / Flan-T5 / HuggingFace Transformers**
- Future: **ONNX** / **TensorRT** optimized **local inference**

### ğŸš€ Deployment
- **Docker-ready**
- Deploy backend to **Render.com**
- Deploy frontend to **Vercel**


---

## ğŸ’¡ Why FinChain?

Traditional expense trackers die because:

- You have to **explicitly categorize** every entry.
- The UI/UX is boring, repetitive, and forces you to adapt.
- Multi-language, unstructured chat input is _never_ supported.

**FinChain flips this:**

- _You_ write in your own words, any language.
- The AI handles parsing, categorizing, and presenting analytics.
- You stay consistent, get value, and enjoy the process!

---


## ğŸ§  How it Works

### 1. **NLP-Driven Parsing**

- **Input:** You type _any expense in any language_.
- **LLM Chain:**  
  The backend uses Hugging Face Transformers (e.g., flan-t5-small, and upgradable) through LangChain, and runs inference (soon via ONNX+TensorRT for speed) to extract:
  - **item:** What did you spend on?
  - **amount:** How much?
  - **category:** What type of expense? (Food, Transport, Shopping, etcâ€”learned by the AI).
- **Storage:**  
  Parsed results and original text are stored in **MongoDB** (fully queryable, exportable).
- **Analytics:**  
  Realtime analytics and charts are auto-generated from the parsed/categorized data.
- **(Planned) Semantic Search:**  
  Pinecone vector DB integration allows â€œfuzzyâ€ AI queries like _â€œShow me all Uber rides this yearâ€_ even if your input text never matched â€œUberâ€ exactly.

---

## âš™ï¸ Stack

| Layer    | Tech                                                |
| -------- | --------------------------------------------------- |
| Backend  | FastAPI (Python, async REST)                        |
| AI/NLP   | Hugging Face Transformers, LangChain, ONNX/TensorRT |
| Storage  | MongoDB (expenses), Pinecone (vector search)        |
| Frontend | Next.js (React), Tailwind, Recharts, Lucide         |
| Deploy   | Docker, Vercel/Render (planned)                     |

---

## ğŸš¦ Current Status

- [x] **Fully working backend:** Expense parsing via NLP, saving to MongoDB.
- [x] **Modern dashboard UI:** Analytics, charts, and transaction feed.
- [x] **No manual categories:** LLM auto-categorizes entries, even in Bangla.
- [x] **All APIs connected:** Frontend-backend integration complete.
- [x] **Data visualizations:** Pie/bar charts, summary cards.
- [x] **NLP extraction is language-agnostic.**
- [x] **Pinecone and ONNX/TensorRT groundwork complete.**
- [ ] **(Next Up)** Semantic search, budget/goal modules, more advanced analytics.

---

## ğŸ“… Whatâ€™s Next

- **ONNX/TensorRT** for GPU-accelerated, low-latency inference.
- **Budget and goal setting**: set, track, and visualize financial targets.
- **Semantic AI Search**: Ask â€œHow much did I spend on food in June?â€â€”get an answer instantly.
- **Auto-learning categories**: Model adapts to your spending style.
- **Mobile-first responsive UI.**
- **Cloud deployment demo (Render/Vercel).**
- **Export, sharing, multi-user support.**

---

## ğŸš€ Getting Started

1. **Clone the repo & install:**

   ```bash
   git clone https://github.com/yourusername/finchain.git
   cd finchain
   ```

2. **Backend:**

   - Install dependencies:
     ```bash
     cd backend
     python -m venv venv
     source venv/bin/activate  # or venv\Scripts\activate
     pip install -r requirements.txt
     ```
   - Set up your `.env`:
     ```
     MONGO_URI=mongodb+srv://...       # Your MongoDB Atlas URI
     GROQ_API_KEY=...                  # Or Hugging Face key for LLM inference
     PINECONE_API_KEY=...              # (Optional for semantic search)
     ```
   - Run FastAPI:
     ```bash
     uvicorn main:app --reload
     ```

3. **Frontend:**
   - Install dependencies:
     ```bash
     cd frontend
     npm install
     ```
   - Run Next.js app:
     ```bash
     npm run dev
     ```
   - Go to [http://localhost:3000](http://localhost:3000)

---

## ğŸ› ï¸ Project Structure
```
finchain/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                # FastAPI entrypoint
â”‚   â”œâ”€â”€ db.py                  # MongoDB functions
â”‚   â”œâ”€â”€ nlp.py                 # LLM inference logic (LangChain, etc)
â”‚   â”œâ”€â”€ pinecone.py            # Vector DB sync
â”‚   â””â”€â”€ ...                    # More modules
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ page.tsx           # Dashboard page (all React logic)
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â””â”€â”€ AnalyticsDashboard.jsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ api.js             # API helpers for frontend-backend
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ ...                # Static files, favicon, etc
â”‚   â””â”€â”€ ...                    # Next.js setup
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ ...
```
---

## ğŸ§‘â€ğŸ’» Contributing

PRs welcome! See `CONTRIBUTING.md` or open an issue.

---

## ğŸ“œ License

MIT

---

## ğŸ† Credits

- [Sabik Aftahee](https://github.com/synuso) (Lead dev, NLP/backend/frontend)
- [OpenAI, Hugging Face, Pinecone](https://huggingface.co) (AI/infra)

---

## ğŸ“· Attachments

Screenshots:

-![Dashboard](https://github.com/user-attachments/assets/b647fbb8-bc3b-4800-bf7e-5f3921ccadad)
-![Rransaction Reports](https://github.com/user-attachments/assets/752ded8a-62f4-49a7-9b06-7dd5694a516b)
-![speinding categories](https://github.com/user-attachments/assets/aa68408a-74aa-41e1-9494-9eb1efa740e1)


**FinChain â€” Track expenses the way _you_ actually talk. Stop categorizing. Start understanding.**
